{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Neural Network for Classifying Handwritten Digits\n",
    "\n",
    "The data set we will be using is called [MNIST](http://yann.lecun.com/exdb/mnist/). It was create by Yann Lecun and is considered to be the \"hello world\" of datasets for deep learning. \n",
    "\n",
    "MNIST consists of 28x28 pixel images of hand written digis.\n",
    "\n",
    "<center> <img src=\"img/mnist.png\" width=\"300\"/>  </center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "We are going to create a feed forward neural network to classify the digits. This will be a simple network composed of linear layers and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transforms\n",
    "\n",
    "When we get the dataset, we will be given tuples that look like:\n",
    "\n",
    "        (PIL image, label)\n",
    "\n",
    "We will need to apply a transform to each image loaded to convert it to a tensor. This can be done using a library included with Pytorch called TorchVision. This library contains datasets, helper functions, transforms, and even whole neural networks that can all be loaded easily.\n",
    "\n",
    "We will use the transform `ToTensor()` which will convert each image to a tensor that we can use in our model. There are lots of other transforms that we have access to such as rotating or flipping images.\n",
    "\n",
    "The `Compose` method is how you string several transforms together. They will be executed in the order they are put in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])  # Try with no normalization for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision has several datasets builtin. These functions make it easy to download and create a dataset from these commonly used datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=xforms)\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=xforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the datasets are created we use them to instantiate the dataloader. A dataloader is a feature of pytorch that allows you to easily batch and loop over datasets. These can be cusomized by simply writing your own dataset class and feeding it to the dataloader. \n",
    "\n",
    "Dataloaders allow you to control the batch size, number of threads for loading, shuffling, collating and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at the data\n",
    "\n",
    "Pytorch data loaders are a convenient way to get tensors into your model with very little manual work. Custom datasets are very easy to define if you are using data that doesnt fit well into one of the predefined classes.\n",
    "\n",
    "The dataloaders defined above work just like an iterator. We can get an item from it by calling `next(iter(loader))`. When accessing all of the data we simply have to loop over the dataloader `for data, label in dataloader:`.\n",
    "\n",
    "Lets take a look at how the dataloader returns data to us. For the MNIST dataset (and most imagery data) it returns a tuple where the first element is a  tensor with the shape [batch, channels, height, width], and the second element is the label (in this case an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "d = next(iter(train_loader))\n",
    "print(d[0].shape) # the data\n",
    "print(d[1].shape) # the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 8, 4, 1, 4, 0, 5, 3, 1, 8, 4, 4, 1, 3, 8, 0, 6, 4, 5, 7, 7, 7, 9,\n",
       "        3, 9, 8, 9, 9, 0, 5, 8, 3, 1, 6, 0, 4, 4, 8, 8, 0, 1, 0, 1, 8, 1, 6, 8,\n",
       "        8, 9, 5, 3, 0, 3, 7, 4, 1, 2, 1, 2, 4, 7, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x120e192d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d[0][0].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(in_features=784, out_features=128),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(in_features=128, out_features=64),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(in_features=64, out_features=10),\n",
    "                            torch.nn.Softmax(dim=1)\n",
    "                        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    " 1. Why is the input size 784?\n",
    " 2. Why is the output size 10?\n",
    " 3. Why are we using Softmax on the last layer instead of ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to_device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Optimizer\n",
    "\n",
    "We will be using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Loss function\n",
    "\n",
    "For classification problems in general you will use the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | 6.90 s - loss: 1.5983443323737268\n",
      "Epoch 1 | 7.20 s - loss: 1.5141877420167171\n",
      "Epoch 2 | 7.30 s - loss: 1.5050633366682382\n",
      "Epoch 3 | 7.37 s - loss: 1.498974173435016\n",
      "Epoch 4 | 7.61 s - loss: 1.4974307021352529\n",
      "Epoch 5 | 7.76 s - loss: 1.4943976650105866\n",
      "Epoch 6 | 7.82 s - loss: 1.4942159629833978\n",
      "Epoch 7 | 8.05 s - loss: 1.4927045101804266\n",
      "Epoch 8 | 8.09 s - loss: 1.4904928813611011\n",
      "Epoch 9 | 8.17 s - loss: 1.490079967833277\n",
      "Epoch 10 | 8.19 s - loss: 1.491015525769069\n",
      "Epoch 11 | 8.23 s - loss: 1.4939467125355816\n",
      "Epoch 12 | 8.24 s - loss: 1.490795740567799\n",
      "Epoch 13 | 8.23 s - loss: 1.4896018459344469\n",
      "Epoch 14 | 8.21 s - loss: 1.4892251004796546\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "loss_history = []\n",
    "for epoch in range(loaded_epoch, epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_time = time.time()\n",
    "    for data, labels in train_loader:\n",
    "        # Flatten the tensors to go from 28x28 -> 784\n",
    "        flat_data = data.view(data.shape[0], -1).to_device(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(flat_data)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    loss_history.append(epoch_loss/len(train_loader))    \n",
    "    print(f\"Epoch {epoch} | {time.time()-epoch_time:.2f} s - loss: {epoch_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x120ed8350>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct/Total: 9639/10000\n",
      "Accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        flat_data = data.view(data.shape[0], -1)\n",
    "        output = model(flat_data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_count += pred.eq(label.view_as(pred)).sum().item()\n",
    "        \n",
    "print(f\"Correct/Total: {correct_count}/{len(test_loader.dataset)}\")\n",
    "print(f\"Accuracy: {correct_count/len(test_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\": model.state_dict(), \n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(path)\n",
    "model.load_state_dict(weights['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untrained Model\n",
    "\n",
    "Lets take a look at how the model does if we dont train it. Lets create a new model and run it on the validation set.\n",
    "\n",
    "### BEFORE YOU RUN\n",
    "\n",
    "### What do you expect the accuracy of the model to be?\n",
    "Hint: How many classes are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct/Total: 1211/10000\n",
      "Accuracy: 0.1211\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "untrained.eval()\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        flat_data = data.view(data.shape[0], -1)\n",
    "        output = untrained(flat_data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_count += pred.eq(label.view_as(pred)).sum().item()\n",
    "        \n",
    "print(f\"Correct/Total: {correct_count}/{len(test_loader.dataset)}\")\n",
    "print(f\"Accuracy: {correct_count/len(test_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
