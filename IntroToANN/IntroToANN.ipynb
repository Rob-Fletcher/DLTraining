{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Artificial Neural Networks\n",
    "\n",
    "<br/>\n",
    "\n",
    "    1. Perceptron and Feed-forward Networks\n",
    "    2. Optimzation, the Loss, and Backpropagation\n",
    "    3. Survey of Common Types of ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron and Feed-forward Networks\n",
    "    \n",
    "                        \n",
    "<center><img src=\"img/perceptron.png\" width=\"240\" height=\"240\"></center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "The simplest form of the ANN is the perceptron. One single unit that performs an operation on inputs and returns a single output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/perceptron.png\" width=\"240\" height=\"240\"/></center>\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "f(w_1 x_1 + w_2 x_2 + ... + w_N x_N + b) =  f\\left(b + \\sum_{i=1}^{N} w_i x_i \\right) =  y\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "$f(x)$ is called the activation function and $b$ is the bias. In principal this could be anything but it typically some non-linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Layer Perceptron\n",
    "\n",
    "We can stack these perceptrons together to introduce more parameters and more non-linear behavior\n",
    "\n",
    "$$\n",
    "\\large\n",
    "f\\left(b + \\sum_{i=1}^{N} w_{i} h_i(x) \\right) = y\n",
    "$$\n",
    "\n",
    "where $h_i(x)$ can be another set of perceptrons like in the previous slides.\n",
    "\n",
    "<center><img src=\"img/twolayer.png\" width=\"600\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "The $f(x)$ we used earlier introduces non-linearity into the perceptron. Without non-linearities a multilayer perceptron would reduce to a single layer perceptron through a simple redefinition of parameters (the $w$s).\n",
    "\n",
    "<center><img src=\"img/activations.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization, the Loss, and Backpropagation\n",
    "\n",
    "\n",
    "### Can we find the parameters $w_{ij}$ such that for an input $x$ we get the answer we want $y$?\n",
    "\n",
    "\n",
    "This seems like a simple optimization problem. Find the $w_{ij}$ that minimize (**insert thing to minimize here**). \n",
    "\n",
    "But what do we minimize? How do we find where that thing is a minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/simpleGD.png\" width=\"500\"/></center>\n",
    "\n",
    "Lots of methods exist for \"Gradient Descent\" minimization. All modern methods use derivatives to estimate the slope in some region, then take a step \"downhill\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Loss\n",
    "\n",
    "The thing to minimize we call the loss or loss function. It is simply a measure of distance between what the output of our network is and what we think it should be for a given input.\n",
    "\n",
    "The simplest loss function you could have is the **Mean Absolute Error (MAE)** \n",
    "\n",
    "$$\n",
    "\\Large\n",
    "MAE = |y-\\hat{y}|\n",
    "$$\n",
    "\n",
    "Here $y$ is the \"target\" or what we want the network to output, and $\\hat{y}$ is the actual output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now we just need to iteratively take steps towards the bottom of the loss.\n",
    "\n",
    "This is done by calculating the gradient at your current point then taking a step in the direction of the steepest slope. Lets consider a two layer perceptron with a single input and output.\n",
    "\n",
    "<center><img src=\"img/simpleExample.svg\" width=\"250\"/></center>\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "h(x) = f(b_1 + w_1 x)\n",
    "$$ \n",
    "\n",
    "is the value of the hidden unit, and\n",
    "\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\hat{y} = f( b_2 + w_2 h(x)) \n",
    "$$ \n",
    "\n",
    "is the value of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The update rule for our weights is,\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "w_{i+1} = w_i - \\epsilon \\frac{\\partial L}{\\partial w}; \\space \\space b_{i+1} = b_i - \\epsilon \\frac{ \\partial L }{ \\partial b}\n",
    "$$\n",
    "\n",
    "So we need to find the gradients of the loss with respect to the parameters of the network. For a one layer network this is easy, but for deeper networks we can use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\large\n",
    "\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "where we have set $\\large z_2 = b_2 + w_2 h(x)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center><img src=\"img/backprop.svg\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### These are all the components needed to train a neural network\n",
    "\n",
    "    1. Linear combination of weights\n",
    "    2. Non-linear activation function\n",
    "    3. Derivatives of Loss function with respect to all parameters\n",
    "    \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "**Note:** The way computers calculate derivates is through a process called **Auto-differentiation**. It uses the chain rule to split the derivative up into smaller parts until it can evaluate a math primative. This is very similar to what we want to do with backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Survey of Common Types of ANNs\n",
    "## Feed Forward Network\n",
    "\n",
    "Any combination of preceptrons arranged in layers.\n",
    "<center><img src=\"img/feedforward.png\" width=\"300\"></center>\n",
    "\n",
    "Common uses are classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "<center> <img src=\"img/autoencoder.png\" width=\"300\"/> </center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Unsupervised learning** - The inputs are used as the labels.\n",
    "\n",
    "\n",
    "Can be used as a learned compression for a specific type of data. Can be used as an anomoly detector by selecting inputs that have a large reconstruction error.\n",
    "\n",
    "There are a ton of variations on autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## De-noising Autoencoders\n",
    "\n",
    "\n",
    "<center> <img src=\"img/DAE.png\" /> </center>\n",
    "\n",
    "### Can learn to remove noise from inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "<center> <img src=\"img/RNN.png\" width=\"500\"/> </center>\n",
    "\n",
    "The output of the node (the hidden state) is concatenated with the next input. This recurrence allows the network to learn patterns in sequences of data.\n",
    "\n",
    "Commonly used in NLP to generate sentences or translate from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "<center><img src=\"img/CNN.png\"></center>\n",
    "\n",
    "CNNs have their weights arranged into \"filters\". This makes them preserve spatial nature of the inputs. The easiest way to see how this works is to look at an example:  \n",
    "<center><a href=\"http://setosa.io/ev/image-kernels/\" target=\"_blank\">Image Kernels</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Every filter is passed over the image to produce a feature map.\n",
    "\n",
    "<center><img src=\"img/CNNVolume.png\" width=\"500\"/></center>\n",
    "\n",
    "### The next layer then passes a new set of filters over the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/features.png\" width=\"900\"/><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next time... Pytorch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
