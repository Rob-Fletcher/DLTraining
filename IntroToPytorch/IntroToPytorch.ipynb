{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Pytorch\n",
    "\n",
    "<br/>\n",
    "\n",
    "    1. Pytorch Tensor Library\n",
    "    2. Computation Graphs and Autograd\n",
    "    3. Pytorch Modules\n",
    "    4. The torch.optim package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytorch Tensor Library `torch.Tensor`\n",
    "\n",
    "Deep learning frameworks use an object called a **Tensor** to represent structured data. Tensors are a generalization of a matrix that can represent high dimensional data. Tensors have a property called **Rank**. This is the number of indices required to address any single element of the tensor.\n",
    "\n",
    "<center> <img src=\"img/tensor.jpg\" width=\"400\"/></center>\n",
    "\n",
    "Pytorch's tensor API is largely a copy of the Numpy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating Tensors\n",
    "Tensors can be created with python lists as well as several other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 7, 1])\n",
      "tensor([[2, 5],\n",
      "        [3, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([3, 7, 1])\n",
    "print(t)\n",
    "\n",
    "t = torch.tensor([[ 2, 5 ],[ 3, 9 ]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Indexing Tensors\n",
    "\n",
    "Indexing a tensor produces a tensor of lower rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[ 2, 5 ],[ 3, 9 ]])  # rank 2 tensor\n",
    "\n",
    "print(t[0])  #  returns a rank 1 tensor\n",
    "\n",
    "print(t[0][0]) # returns a rank 0 tensor. Equivalently t[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slicing Tensors\n",
    "\n",
    "Like numpy arrays, tensors can be sliced along axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3]])\n",
      "\n",
      "tensor([1, 1, 1])\n",
      "tensor([2, 2, 2])\n",
      "\n",
      "tensor([1, 2, 3])\n",
      "\n",
      "tensor([[1, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,1,1],[2,2,2],[3,3,3]])\n",
    "print(t)\n",
    "print()\n",
    "\n",
    "print(t[0,:]), print(t[1,:])\n",
    "print()\n",
    "\n",
    "print(t[:,0])\n",
    "print()\n",
    "\n",
    "print(t[0:2, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor Shape and Reshaping\n",
    "\n",
    "The shape of a tensor tells you the size along each of its dimensions. The length of the shape is the tensors rank. Tensors can be reshaped into a different rank with the same total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4])\n",
      "tensor([[1, 1, 1, 2, 2, 2],\n",
      "        [3, 3, 3, 4, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,1,1],[2,2,2],[3,3,3],[4,4,4]])\n",
    "print(t.shape)\n",
    "\n",
    "r = t.view(12)\n",
    "print(r)\n",
    "\n",
    "r = t.view(2,-1)  # -1 tell the tensor to figure out what number should go there based on the number of elements\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "There are a large number of math operations and other miscalanious operations available to tensors. Normal math operations (+, \\*, etc...) **<span style=\"color:red\">Always Operate Elementwise</span>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,1])\n",
    "y = torch.tensor([2,2])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Note**: This is not the case for operations that specifically define how elements operate, e.g. matrix multiplication, concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x,y])  #concatenate along the first axis\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor Broadcasting (__Advanced Topic__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,2)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "But doesnt this break our rule that tensors operate elementwise?   \n",
    "\n",
    "\n",
    "Rank 2 * Rank 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is actually happening in our example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,2)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,2)) * torch.tensor([[2.,2],[2,2],[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Broacasting replicates the scalar enough times to have the same number of elements as the first tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor Broadcasting (Advanced Topic)\n",
    "\n",
    "Broadcasting is a method by which tensors of different size or rank can operate together.\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    " * Each tensor has at least one dimension.\n",
    "\n",
    " * When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((2,3,2))\n",
    "y = torch.ones((   3,2))\n",
    "\n",
    "print(x+y)\n",
    "\n",
    "x = torch.zeros((2,3,2))\n",
    "y = torch.ones(( 1,3,2))\n",
    "print(x+y)\n",
    "\n",
    "x = torch.zeros((2,3,2))\n",
    "y = torch.ones((     2))\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ff950f9dcf68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((2,3,2))\n",
    "y = torch.ones((   3,3))\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation Graphs and Autograd\n",
    "\n",
    "Automatic Differentiation is the core of every deep learning library (Called **Autograd** in Pytorch).\n",
    "     It is used to calculate the derivatives (gradients) of a function by using the chain rule and black magic. \n",
    " \n",
    "Training every nerual network consists of two phases:\n",
    "   1. **Forward pass** - calculate output of the network starting at the inputs\n",
    "   2. **Backward pass** - backpropagation to compute all of the gradients starting with the output\n",
    "     \n",
    "All math operations in the **forward pass** are represented by a **Computation Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representing Computation as a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = 10 - d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src=\"img/computation_graph_forward.png\" width=\"400\"> </center>\n",
    "\n",
    "[Example from here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The gradients we need to find are:\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial w_3}, \\frac{\\partial L}{\\partial w_4}\n",
    "$$\n",
    "\n",
    "In other words, how does the network output ($L$, or the loss) change as we change each of our parameters ($w_i$ or the weights).\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Our job when writing a pytorch model is to define the computation graph so that the **Autograd** library can find these gradients automatically.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytorch Autograd - `torch.nn.Autograd`\n",
    "\n",
    "In pytorch, the gradients and the associated operations are kept track of inside of each tensor if its member `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones((2,2), requires_grad=True)\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([1.], requires_grad=True)\n",
    "w1 = torch.tensor([1.], requires_grad=True)\n",
    "w2 = torch.tensor([1.], requires_grad=True)\n",
    "w3 = torch.tensor([1.], requires_grad=True)\n",
    "w4 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = 10 - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.], grad_fn=<MulBackward0>), tensor([2.], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The backward functions are stored in the tensors themselves. There is no \"central\" representation of the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytorch Functions - `torch.nn.Autograd.Function`\n",
    "\n",
    "All math operations are defined in the `Function` class. `Function` has two important methods:\n",
    "\n",
    "  * `forward` - Simply computes the output of the function given the inputs\n",
    "  * `backward` - Take gradients from later parts of the computation graph and computes the gradients relative to the inputs. It then sends these gradients to earlier parts of the computation graph and invokes their `backward` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.], grad_fn=<RsubBackward1>)\n",
      "tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "print(L)\n",
    "L.backward()\n",
    "print(w4.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytorch vs. Tensorflow\n",
    "\n",
    "In Tensorflow the computation graph is defined at initialization and the graph is run by simply passing all the inputs through it. \n",
    "\n",
    "In Pytorch the computation graph is built up from the operations contained in the tensors. After a call to `backward` all `grad_fn` references are destroyed. The next forward pass of the graph recreates each of these. This means the computation graph can change at run time.\n",
    "\n",
    "###         **Tensorflow** - Static Computation Graph\n",
    "\n",
    "###         **Pytorch** - Dynamic Computation Graph\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "###### * Tensorflow 2.0 uses eager execution which gives it some dynamic graph capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we know about tensors, computation graphs and how to calculate gradients. Lets move on to how we will build our graphs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytorch Modules - `torch.nn.Module`\n",
    "\n",
    "In Pytorch we create computation graphs with Modules. There are many prebuilt ones but the `Module` class allows us to create our own as well.\n",
    "\n",
    "Neural networks tend to have their computation graph organized into layers. This is the same as how we stacked layers of perceptrons previously. A layer of perceptrons, which is a linear combination of its inputs, is implemented in a module conveniently called `Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8849,  0.4273, -2.3330,  1.4144,  1.0879], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = torch.nn.Linear(in_features=3, out_features=5)\n",
    "\n",
    "x = torch.tensor([1.,2.,3.])\n",
    "lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layers can be stacked together\n",
    "\n",
    "The output of one layer can be fed into the input of another. This is how we create the stacked structure of a typical neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6024, -0.5019,  1.1678,  0.9438], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.,2.,3.])\n",
    "\n",
    "lin1 = torch.nn.Linear(in_features=3, out_features=5)\n",
    "lin2 = torch.nn.Linear(in_features=5, out_features=4)\n",
    "\n",
    "hidden = lin1(x)\n",
    "output = lin2(hidden)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using `Module` to Build a Network\n",
    "\n",
    "Modules can contain other modules. This is how we build up more complicated graphs and keep them in easy to use packages. To make a module you only need to overload the `__init__()` and `forward()` functions. Lets also throw in some activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCoolNet(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(MyCoolNet, self).__init__()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(in_size, 10)\n",
    "        self.lin2 = torch.nn.Linear(10, 15)\n",
    "        self.lin3 = torch.nn.Linear(15, 10)\n",
    "        self.lin4 = torch.nn.Linear(10, out_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.relu(self.lin1(x))\n",
    "        hidden = self.relu(self.lin2(hidden))\n",
    "        hidden = self.relu(self.lin3(hidden))\n",
    "        output = self.sig(self.lin4(hidden))\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we can use our network just like we used the `Linear` layer earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4050, 0.4399, 0.5060, 0.5242, 0.4559], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyCoolNet(in_size=5, out_size=5)\n",
    "\n",
    "x = torch.tensor([1.,2,3,4,5])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now build computation graphs, send tensors through them, and calculate their gradients. All thats left is to use the gradients to adjust the weights.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The `torch.optim` Package\n",
    "\n",
    "We have explored tensors, computation graphs and the ability to calculate their gradients. Now we need to adjust the weights of the network based on their gradients. This is done with the `torch.optim` package.\n",
    "\n",
    "This package contains all of the optimizers that are available for use in Pytorch, as well as the framework to make new ones.\n",
    "\n",
    "We will explore in detail an optimizer called **Stochastic Gradient Descent** (**SGD**) and then briefly describe more modern optimizers that are modifications of **SGD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Remember from last time, we said that our update rule for the parameters of our graph (weights) was:\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "w_{t+1} = w_t - \\alpha \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "In words, for each parameter in in the graph, the current value $w_{i+1}$ is the previous value $w_i$ minus the gradient of $L$.\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "This is the basic form of **Stochastic Gradient Descent**. It is stochastic because, in general, $\\partial L / \\partial w$ is the average gradient over a random subset of inputs from our dataset. We will focus on a particular variation of **SGD** called **SGD with Momentum**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Momentum\n",
    "\n",
    "To make **SGD** learn faster and be less likely to get stuck in local minima we can use a moving average. In this case we will use an exponentially weighted moving average.\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "V_t = \\beta V_{t-1} + (1-\\beta) \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$V_t$ is called the momentum. We can now write our parameter update rule as:\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\Large\n",
    "w_{t+1} = w_t - \\alpha V_t\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "Alpha is what is called the learning rate. Momentum has the effect of 'building up speed' to carry the optimizer through areas of low gradient where normally it would slow down more than desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers in Pytorch\n",
    "\n",
    "To use **SGD** in Pytorch we just need to instantiate its class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCoolNet(in_size=5, out_size=5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  \n",
    "# If we give no argument for momentum then standard SGD is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice for us to initialize the optimizer we must pass the parameters of our model. This is so the optimizer knows which weights to adjust at each step.\n",
    "\n",
    "We can now use this optimizer to minimize a loss function on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimizing a Loss Function\n",
    "\n",
    "Lets create a simple loss function and adjust the weights one step in the direction of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0: 1.3889371156692505\n",
      "Loss 1: 1.3281832933425903\n",
      "Loss 2: 1.2424967288970947\n",
      "Loss 3: 1.1397792100906372\n",
      "Loss 4: 1.0178158283233643\n",
      "Loss 5: 0.8763992190361023\n",
      "Loss 6: 0.7107219099998474\n",
      "Loss 7: 0.5144205093383789\n",
      "Loss 8: 0.2940930128097534\n",
      "Loss 9: 0.10847821831703186\n"
     ]
    }
   ],
   "source": [
    "model = MyCoolNet(in_size=5, out_size=5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.8)\n",
    "\n",
    "x = torch.tensor([1.,2,3,4,5])\n",
    "\n",
    "for i in range(10):\n",
    "    out = model(x)\n",
    "    loss = torch.sum((0-out)**2)  #loss decreases as out->0\n",
    "\n",
    "    loss.backward()   # Calculate gradients\n",
    "    optimizer.step()  # Step all weights in the direction of gradient\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Loss {i}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modern Optimizers\n",
    "\n",
    "There are many variations on **SGD** that incorporate some sort of momentum. While these optimizers still show state of the art performance in some tasks, they take more care to get good performance. Newer adaptive methods solve many of the problems with **SGD**.\n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/SJVFO3IcVC0M0/giphy.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Adam** \n",
    "### A modification of SGD that uses the first and second moments of the gradient to adjust the learning rate. Adam calculates these moments and adjusts the learning rate for each parameter. Building a new network? Start with Adam `torch.optim.Adam()`.\n",
    "  \n",
    "$$\n",
    "\\Large\n",
    "w_{t+1} = w_t - \\eta \\frac{\\hat{m}_t}{ \\sqrt{\\hat{v}_t} + \\epsilon }\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\large\n",
    "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\large\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **LookAhead** \n",
    "#### Uses a combination of two optimizers, a fast and slow one. The fast one explores the loss ahead of the slow one and after a number of iterations, the slow optimizer is then moved to the linear interpolation between the two. The fast optimizer could be any other optimizer. SGD, Adam, etc...\n",
    "\n",
    "<center> <img src=\"img/lookahead.png\" width=\"500\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "We have taken a look at the basic components of Pytorch and how they can be combined to form computation graphs. We then looked at how to optimize our paramters with gradient descent and got a preview of modern optimizers.\n",
    "\n",
    "<center> <img src=\"https://media.giphy.com/media/1PNcBiyhzL0Na/giphy.gif\"/> </center>\n",
    "\n",
    "## Next time...  Hands on with Pytorch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
