{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Artificial Neural Networks\n",
    "\n",
    "<br/>\n",
    "\n",
    "    1. Perceptron and Feed-forward Networks\n",
    "    2. Optimzation, the Loss, and Backpropagation\n",
    "    3. Survey of Common Types of ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron and Feed-forward Networks\n",
    "    \n",
    "                        \n",
    "<center><img src=\"img/perceptron.png\" width=\"240\" height=\"240\"></center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "The simplest form of the ANN is the perceptron. One single unit that performs an operation on inputs and returns a single output. Here the $w_x$'s are called the weights or sometimes the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/perceptron.png\" width=\"240\" height=\"240\"/></center>\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "f(w_1 x_1 + w_2 x_2 + ... + w_N x_N + b) =  f\\left(b + \\sum_{i=1}^{N} w_i x_i \\right) =  y\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "$f(x)$ is called the activation function and $b$ is the bias. In principal $f$ could be anything but is typically some non-linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Layer Perceptron\n",
    "\n",
    "We can stack these perceptrons together to introduce more parameters and more non-linear behavior\n",
    "\n",
    "$$\n",
    "\\large\n",
    "f\\left(b + \\sum_{i=1}^{N} w_{i} h_i(x) \\right) = y\n",
    "$$\n",
    "\n",
    "where $h_i(x)$ can be another set of perceptrons like in the previous slides.\n",
    "\n",
    "<center><img src=\"img/twolayer.png\" width=\"600\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Layer Perceptron\n",
    "\n",
    "Notice that our definition of a perceptron is the same as matrix multiplication. From before we had:\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "f\\left(b_i + \\sum_{i=1}^{N} w_{ij} x_i \\right) = y_i \n",
    "$$\n",
    "<br/>\n",
    "Now as matrices:\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "W = \\begin{bmatrix} \n",
    "    w_{11} & w_{12} & \\dots \\\\\n",
    "    \\vdots & \\ddots &  \\\\\n",
    "    w_{i1} &        & w_{ij} \\\\\n",
    "\\end{bmatrix}\n",
    ";\\quad\n",
    "X = \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_i\n",
    "\\end{bmatrix}\n",
    "\\quad \\quad \n",
    "f\\left(B + WX\\right) = Y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "The $f(x)$ we used earlier introduces non-linearity into the perceptron. Without non-linearities a multilayer perceptron would reduce to a single layer perceptron through a simple redefinition of parameters (the $w$s).\n",
    "\n",
    "<center><img src=\"img/activations.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cool. So What?\n",
    "\n",
    "### You can imagine that there exists a set of parameters W, the input of \"cat\" will return a value corresponding to the classification \"cat\".\n",
    "\n",
    "<center><img src=\"img/cat_equation.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization, the Loss, and Backpropagation\n",
    "\n",
    "\n",
    "### Can we find the parameters $w_{ij}$ such that for an input $x$ we get the answer we want $y$?\n",
    "\n",
    "\n",
    "This seems like a simple optimization problem. Find the $w_{ij}$ that minimize (**insert thing to minimize here**). \n",
    "\n",
    "But what do we minimize? How do we find where that thing is a minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"img/simpleGD.png\" width=\"500\"/></center>\n",
    "\n",
    "To find the bottom we can simply take a series of small steps \"downhill\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Loss\n",
    "\n",
    "The thing to minimize we call the loss or loss function. It is simply a measure of distance between what the output of our network is and what we think it should be for a given input.\n",
    "\n",
    "The simplest loss function you could have is the **Mean Absolute Error (MAE)** \n",
    "\n",
    "$$\n",
    "\\Large\n",
    "MAE = |y-\\hat{y}|\n",
    "$$\n",
    "\n",
    "Here $y$ is the \"target\" or what we want the network to output, and $\\hat{y}$ is the actual output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"img/loss.png\" width=\"400\"></td>\n",
    "    <td>\n",
    "        <table>\n",
    "            <tr>\n",
    "                $$\n",
    "                  \\Large L1 = | y - \\hat y |\n",
    "                $$\n",
    "            </tr>\n",
    "            <tr>\n",
    "                $$\n",
    "                \\Large L2 = (y - \\hat y)^2\n",
    "                $$\n",
    "            </tr>\n",
    "<tr>        \n",
    "$$\n",
    "\\Large L_{\\epsilon} =\n",
    "   \\begin{cases} \n",
    "     |y - \\hat y| & \\text{if $|y- \\hat y| < \\epsilon$} \\\\ \n",
    "     0 & \\text{otherwise} \n",
    "\\end{cases} \n",
    "$$\n",
    "            </tr>\n",
    "        </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now we just need to iteratively take steps towards the bottom of the loss.\n",
    "\n",
    "This is done by calculating the gradient at your current point then taking a step in the direction of the steepest slope. Lets consider a two layer perceptron with a single input and output.\n",
    "\n",
    "<center><img src=\"img/simpleExample.svg\" width=\"250\"/></center>\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "h(x) = f(b_1 + w_1 x)\n",
    "$$ \n",
    "\n",
    "is the value of the hidden unit, and\n",
    "\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\hat{y} = f( b_2 + w_2 h(x)) \n",
    "$$ \n",
    "\n",
    "is the value of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The update rule for our weights is,\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\large\n",
    "w_{t+1} = w_t - \\epsilon \\frac{\\partial L}{\\partial w_t}; \\space \\space b_{t+1} = b_t - \\epsilon \\frac{ \\partial L }{ \\partial b_t}\n",
    "$$\n",
    "\n",
    "Where $L$ is our loss function. So we need to find the gradients of the loss with respect to the parameters of the network. For a one layer network this is easy, but for deeper networks we can use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\large\n",
    "\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "where we have set $\\large z_2 = b_2 + w_2 h(x)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center><img src=\"img/backprop.svg\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we simply have to take a step in the direction of the gradient...\n",
    "\n",
    "<br/>\n",
    "$$\n",
    "\\Large\n",
    "w_{t+1} = w_t - step\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "... then do it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### These are all the components needed to train a neural network\n",
    "\n",
    "    1. Linear combination of weights\n",
    "    2. Non-linear activation function\n",
    "    3. Derivatives of Loss function with respect to all parameters\n",
    "    \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "**Note:** The way computers calculate derivates is through a process called **Auto-differentiation**. It uses the chain rule to split the derivative up into smaller parts until it can evaluate a math primative. This is very similar to what we want to do with backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Survey of Common Types of ANNs\n",
    "## Feed Forward Network\n",
    "\n",
    "Any combination of preceptrons arranged in layers.\n",
    "<center><img src=\"img/feedforward.png\" width=\"300\"></center>\n",
    "\n",
    "Common uses are classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Example\n",
    "\n",
    "<center><img src=\"img/exponen.gif\" width=\"300\"></center>\n",
    "\n",
    "Maybe we want to find the decay parameter in the function $y=e^{-\\frac{x}{\\sigma}}$. In this case the input could be the $y$ value in each $x$ bin with the goal of predicting. Then the output would be our prediction of the parameter $\\hat \\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "<center> <img src=\"img/autoencoder.png\" width=\"300\"/> </center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Unsupervised learning** - The inputs are used as the labels.\n",
    "\n",
    "\n",
    "Can be used as a learned compression for a specific type of data. Can be used as an anomoly detector by selecting inputs that have a large reconstruction error.\n",
    "\n",
    "There are a ton of variations on autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## De-noising Autoencoders\n",
    "\n",
    "\n",
    "<center> <img src=\"img/DAE.png\" /> </center>\n",
    "\n",
    "### Can learn to remove noise from inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "<center> <img src=\"img/RNN.png\" width=\"500\"/> </center>\n",
    "\n",
    "The output of the node (the hidden state) is concatenated with the next input. This recurrence allows the network to learn patterns in sequences of data.\n",
    "\n",
    "Commonly used in NLP to generate sentences or translate from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "<center><img src=\"img/CNN.png\"></center>\n",
    "\n",
    "CNNs have their weights arranged into \"filters\". This makes them preserve spatial nature of the inputs. The easiest way to see how this works is to look at an example:  \n",
    "<center><a href=\"http://setosa.io/ev/image-kernels/\" target=\"_blank\">Image Kernels</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Every filter is passed over the image to produce a feature map.\n",
    "\n",
    "<center><img src=\"img/CNNVolume.png\" width=\"500\"/></center>\n",
    "\n",
    "### The next layer then passes a new set of filters over the feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/features.png\" width=\"900\"/><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net CNN Architecture\n",
    "\n",
    "<center><img src=\"img/unet.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next time... Pytorch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
